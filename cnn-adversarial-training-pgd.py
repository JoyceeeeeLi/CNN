# -*- coding: utf-8 -*-
"""a5q2d.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LdzytN8LFdUr9eaN9EdWouXbxkxaY0KM
"""

# Joyce Li
import tensorflow as tf
import numpy as np
import torch
import random
import torch.nn as nn
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import GaussianNoise
import matplotlib.pyplot as plt
import neural_structured_learning as nsl
import tensorflow_datasets as tfds

# pip install neural_structured_learning

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
print(x_train.shape)
x_train = x_train.astype("float32") / 255
x_test = x_test.astype("float32") / 255

x_train = np.expand_dims(x_train, axis=-1)
x_train = tf.image.resize(x_train, [32,32])

x_test = np.expand_dims(x_test, axis=-1)
x_test = tf.image.resize(x_test, [32,32])

print(x_train.shape)
print(x_test.shape)

y_train = keras.utils.to_categorical(y_train, 10)
y_test = keras.utils.to_categorical(y_test, 10)

input_shape = (32, 32, 1)

### a
model = keras.Sequential(
    [
        keras.Input(shape=input_shape),
        layers.Conv2D(64, kernel_size=(3, 3), activation="relu", padding='same'),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(128, kernel_size=(3, 3), activation="relu",  padding='same'),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(256, kernel_size=(3, 3), activation="relu",  padding='same'),
        layers.Conv2D(256, kernel_size=(3, 3), activation="relu",  padding='same'),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(512, kernel_size=(3, 3), activation="relu",  padding='same'),
        layers.Conv2D(512, kernel_size=(3, 3), activation="relu",  padding='same'),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(512, kernel_size=(3, 3), activation="relu",  padding='same'),
        layers.Conv2D(512, kernel_size=(3, 3), activation="relu",  padding='same'),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dense(4096, activation="relu"),
        layers.Dropout(0.5),
        layers.Dense(4096, activation="relu"),
        layers.Dropout(0.5),
        layers.Dense(10, activation="softmax"),
    ]
)
model.summary()

batch_size = 256
epochs = 5

model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(lr=0.0001), metrics=['accuracy'])
history = model.fit(x_train, y_train, batch_size=batch_size, epochs=5, verbose=1, validation_data=(x_test, y_test), steps_per_epoch=60000/batch_size)
score = model.evaluate(x_test, y_test, verbose=0)
print("Test loss:", score[0])
print("Test accuracy:", score[1])

stepSize = 0.01
eps = 0.2
maxPass = 100
advConfig = nsl.configs.make_adv_reg_config(adv_step_size=stepSize, adv_grad_norm='infinity', pgd_iterations=maxPass, pgd_epsilon=eps)
advModel = nsl.keras.AdversarialRegularization(
  model,
  label_keys=['label'],
  adv_config=advConfig
)

advModel.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(lr=0.0001), metrics=['accuracy'])
history = advModel.fit({'feature': x_train[:30000], 'label': y_train[:30000]}, batch_size=batch_size, epochs=2, verbose=1, steps_per_epoch=30000/batch_size)

score = advModel.evaluate({'feature': x_test, 'label': y_test})
print("Test loss:", score[0])
print("Test accuracy:", score[2])

def FGSM(model, X, y, eps):
  X = tf.cast(X, tf.float32)
  with tf.GradientTape() as tape:
    tape.watch(X)
    pred = model(X)
    loss = keras.losses.categorical_crossentropy(y, pred)
  gradient = tape.gradient(loss, X)
  signedGradient = tf.sign(gradient)
  adv = (X + (signedGradient*eps)).numpy()
  return adv

## FGSM Attack 0.2
adv1 = FGSM(model, x_test, y_test, 0.2)
score = advModel.evaluate({'feature': adv1, 'label': y_test})
print("Test loss:", score[0])
print("Test accuracy:", score[2])

# random samples
idx = []
for i in range(5):
  idx.append(random.randrange(len(adv1)))

fig = plt.figure(figsize=(20, 20))
columns = 5
rows = 1
for i in range(1, columns*rows +1):
  fig.add_subplot(rows, columns, i)
  img = adv1[idx[i-1]].reshape(32,32)
  label = "label: {}".format(np.argmax(y_test[idx[i-1]]))
  plt.xlabel(label)
  plt.imshow(img)
plt.show()

## FGSM Attack 0.1
adv11 = FGSM(model, x_test, y_test, 0.1)
score = advModel.evaluate({'feature': adv11, 'label': y_test})
print("Test loss:", score[0])
print("Test accuracy:", score[2])

# random samples
idx = []
for i in range(5):
  idx.append(random.randrange(len(adv11)))

fig = plt.figure(figsize=(20, 20))
columns = 5
rows = 1
for i in range(1, columns*rows +1):
  fig.add_subplot(rows, columns, i)
  img = adv11[idx[i-1]].reshape(32,32)
  label = "label: {}".format(np.argmax(y_test[idx[i-1]]))
  plt.xlabel(label)
  plt.imshow(img)
plt.show()

## FGSM Attack 0.5
adv15 = FGSM(model, x_test, y_test, 0.5)
score = advModel.evaluate({'feature': adv15, 'label': y_test})
print("Test loss:", score[0])
print("Test accuracy:", score[2])

# random samples
idx = []
for i in range(5):
  idx.append(random.randrange(len(adv15)))

fig = plt.figure(figsize=(20, 20))
columns = 5
rows = 1
for i in range(1, columns*rows +1):
  fig.add_subplot(rows, columns, i)
  img = adv15[idx[i-1]].reshape(32,32)
  label = "label: {}".format(np.argmax(y_test[idx[i-1]]))
  plt.xlabel(label)
  plt.imshow(img)
plt.show()

## PGD Attack 0.2
x_test1 = x_test[:5000]
y_test1 = y_test[:5000]
config = nsl.configs.AdvNeighborConfig(adv_step_size=0.01, adv_grad_norm='infinity', pgd_iterations=100, pgd_epsilon=0.2, feature_mask=None)
with tf.GradientTape() as tape:
      tape.watch(x_test1)
loss = tf.keras.losses.categorical_crossentropy(y_test1, model(x_test1))
adv2 = nsl.lib.gen_adv_neighbor(x_test1, loss, config, gradient_tape=tape, pgd_loss_fn=keras.losses.categorical_crossentropy, pgd_model_fn=model, pgd_labels=y_test1)

score = advModel.evaluate({'feature': adv2[0], 'label': y_test1})
print("Test loss:", score[0])
print("Test accuracy:", score[2])

# random samples
adv22 = np.squeeze(adv2[0].numpy())
idx = []
for i in range(5):
  idx.append(random.randrange(len(adv22)))

fig = plt.figure(figsize=(20, 20))
columns = 5
rows = 1
for i in range(1, columns*rows +1):
  fig.add_subplot(rows, columns, i)
  img = adv22[idx[i-1]]
  label = "label: {}".format(np.argmax(y_test[idx[i-1]]))
  plt.xlabel(label)
  plt.imshow(img)
plt.show()

## PGD Attack 0.1
x_test1 = x_test[:5000]
y_test1 = y_test[:5000]
config = nsl.configs.AdvNeighborConfig(adv_step_size=0.01, adv_grad_norm='infinity', pgd_iterations=100, pgd_epsilon=0.1, feature_mask=None)
with tf.GradientTape() as tape:
      tape.watch(x_test1)
loss = tf.keras.losses.categorical_crossentropy(y_test1, model(x_test1))
adv21 = nsl.lib.gen_adv_neighbor(x_test1, loss, config, gradient_tape=tape, pgd_loss_fn=keras.losses.categorical_crossentropy, pgd_model_fn=model, pgd_labels=y_test1)

score = advModel.evaluate({'feature': adv21[0], 'label': y_test1})
print("Test loss:", score[0])
print("Test accuracy:", score[2])

# random samples
adv211 = np.squeeze(adv21[0].numpy())
idx = []
for i in range(5):
  idx.append(random.randrange(len(adv211)))

fig = plt.figure(figsize=(20, 20))
columns = 5
rows = 1
for i in range(1, columns*rows +1):
  fig.add_subplot(rows, columns, i)
  img = adv211[idx[i-1]]
  label = "label: {}".format(np.argmax(y_test[idx[i-1]]))
  plt.xlabel(label)
  plt.imshow(img)
plt.show()

## PGD Attack 0.5
x_test1 = x_test[:5000]
y_test1 = y_test[:5000]
config = nsl.configs.AdvNeighborConfig(adv_step_size=0.01, adv_grad_norm='infinity', pgd_iterations=100, pgd_epsilon=0.5, feature_mask=None)
with tf.GradientTape() as tape:
      tape.watch(x_test1)
loss = tf.keras.losses.categorical_crossentropy(y_test1, model(x_test1))
adv25 = nsl.lib.gen_adv_neighbor(x_test1, loss, config, gradient_tape=tape, pgd_loss_fn=keras.losses.categorical_crossentropy, pgd_model_fn=model, pgd_labels=y_test1)

score = advModel.evaluate({'feature': adv25[0], 'label': y_test1})
print("Test loss:", score[0])
print("Test accuracy:", score[2])

# random samples
adv255 = np.squeeze(adv25[0].numpy())
idx = []
for i in range(5):
  idx.append(random.randrange(len(adv255)))

fig = plt.figure(figsize=(20, 20))
columns = 5
rows = 1
for i in range(1, columns*rows +1):
  fig.add_subplot(rows, columns, i)
  img = adv255[idx[i-1]].reshape(32,32)
  label = "label: {}".format(np.argmax(y_test[idx[i-1]]))
  plt.xlabel(label)
  plt.imshow(img)
plt.show()

# def PGD(model, X, y, eps, stepSize, maxPass):
#   X = tf.cast(X, tf.float32)
#   upBound = X + np.expand_dims([[eps for i in range(32)] for j in range(32)], axis=-1)
#   lowBound = X - np.expand_dims([[eps for i in range(32)] for j in range(32)], axis=-1)
#   adv = X
#   for i in range(maxPass):
#     # gradient step
#     with tf.GradientTape() as tape:
#       tape.watch(adv)
#       pred = model(adv)
#       loss = keras.losses.categorical_crossentropy(y, pred)
#     gradient = tape.gradient(loss, adv)
#     signedGradient = tf.sign(gradient)
#     adv = (adv + (signedGradient*stepSize))
#     # projection step
#     adv = tf.clip_by_value(adv, lowBound, upBound)
#   return adv.numpy()

# adv2 = np.ndarray(x_test[:2000].shape)
# print(type(adv2))
# for i in range(1, 2):
#   l = (i-1)*1000
#   r = i*1000
#   print("l: {}, r: {}".format(l, r))
#   np.append(adv2, PGD(model, x_test[l:r], y_test[l:r], 0.2, 0.1, 100))