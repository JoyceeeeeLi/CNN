# -*- coding: utf-8 -*-
"""ex3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ts1765iudl6iUI4nh-_IsVCgQTIRJONM
"""

# Joyce Li (20798712)
import tensorflow as tf
import numpy as np
import torch
import random
import torch.nn as nn
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import GaussianNoise
import matplotlib.pyplot as plt

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
print(x_train.shape)
x_train = x_train.astype("float32") / 255
x_test = x_test.astype("float32") / 255
# x_test_1 = x_test
# sample = GaussianNoise(0.1)
# nosiy = sample(x_test_1, training=True)
# plt.imshow(nosiy[0])

### a
x_train = np.expand_dims(x_train, axis=-1)
x_train = tf.image.resize(x_train, [32,32])

x_test = np.expand_dims(x_test, axis=-1)
x_test = tf.image.resize(x_test, [32,32])

print(x_train.shape)
print(x_test.shape)

y_train = keras.utils.to_categorical(y_train, 10)
y_test = keras.utils.to_categorical(y_test, 10)

input_shape = (32, 32, 1)
model = keras.Sequential(
    [
        keras.Input(shape=input_shape),
        layers.Conv2D(64, kernel_size=(3, 3), activation="relu", padding='same'),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(128, kernel_size=(3, 3), activation="relu",  padding='same'),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(256, kernel_size=(3, 3), activation="relu",  padding='same'),
        layers.Conv2D(256, kernel_size=(3, 3), activation="relu",  padding='same'),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(512, kernel_size=(3, 3), activation="relu",  padding='same'),
        layers.Conv2D(512, kernel_size=(3, 3), activation="relu",  padding='same'),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(512, kernel_size=(3, 3), activation="relu",  padding='same'),
        layers.Conv2D(512, kernel_size=(3, 3), activation="relu",  padding='same'),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dense(4096, activation="relu"),
        layers.Dropout(0.5),
        layers.Dense(4096, activation="relu"),
        layers.Dropout(0.5),
        layers.Dense(10, activation="softmax"),
    ]
)
model.summary()

batch_size = 256
epochs = 5

#### b
model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(lr=0.00001), metrics=['accuracy'])
history = model.fit(x_train, y_train, batch_size=batch_size, epochs=5, verbose=1, validation_data=(x_test, y_test), steps_per_epoch=60000/batch_size)

train_loss = model.history.history['loss']
train_acc = model.history.history['accuracy']
test_loss = model.history.history['val_loss']
test_acc = model.history.history['val_accuracy']

epochs = [1, 2, 3, 4, 5]

plt.plot(epochs,train_loss, label='train loss')
plt.legend(loc='lower right')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.show()

plt.plot(epochs, train_acc, label='train acc')
plt.legend(loc='lower right')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.show()

plt.plot(epochs,test_loss, label='test loss')
plt.legend(loc='lower right')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.show()

plt.plot(epochs, test_acc, label='test acc')
plt.legend(loc='lower right')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.show()

### c i 
### flip_left_right
x_test_1 = x_test
x_test_1 = tf.image.flip_left_right(x_test_1)
score = model.evaluate(x_test_1, y_test, verbose=0)
print("flip_left_right:")
print("Test loss:", score[0])
print("Test accuracy:", score[1])


### flip_up_down
x_test_2 = x_test
x_test_2 = tf.image.flip_up_down(x_test_2)
score = model.evaluate(x_test_2, y_test, verbose=0)
print("flip_up_down:")
print("Test loss:", score[0])
print("Test accuracy:", score[1])

### c ii
x_test_3 = x_test
sample = GaussianNoise(1)
nosiy = sample(x_test_3, training=True)
score = model.evaluate(nosiy, y_test, verbose=0)
print("Gaussian noise(1):")
print("Test loss:", score[0])
print("Test accuracy:", score[1])

x_test_4 = x_test
sample = GaussianNoise(0.1)
nosiy = sample(x_test_4, training=True)
score = model.evaluate(nosiy, y_test, verbose=0)
print("Gaussian noise(0.1):")
print("Test loss:", score[0])
print("Test accuracy:", score[1])

x_test_5 = x_test
sample = GaussianNoise(0.01)
nosiy = sample(x_test_5, training=True)
score = model.evaluate(nosiy, y_test, verbose=0)
print("Gaussian noise(0.01):")
print("Test loss:", score[0])
print("Test accuracy:", score[1])

### d
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = x_train.astype("float32") / 255
x_test = x_test.astype("float32") / 255

x_train = np.expand_dims(x_train, axis=-1)
x_train = tf.image.resize(x_train, [32,32])

x_test = np.expand_dims(x_test, axis=-1)
x_test = tf.image.resize(x_test, [32,32])

y_train = keras.utils.to_categorical(y_train, 10)
y_test = keras.utils.to_categorical(y_test, 10)

input_shape = (32, 32, 1)
model = keras.Sequential(
    [
        keras.Input(shape=input_shape),
        layers.Conv2D(64, kernel_size=(3, 3), activation="relu", padding='same'),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(128, kernel_size=(3, 3), activation="relu",  padding='same'),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(256, kernel_size=(3, 3), activation="relu",  padding='same'),
        layers.Conv2D(256, kernel_size=(3, 3), activation="relu",  padding='same'),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(512, kernel_size=(3, 3), activation="relu",  padding='same'),
        layers.Conv2D(512, kernel_size=(3, 3), activation="relu",  padding='same'),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(512, kernel_size=(3, 3), activation="relu",  padding='same'),
        layers.Conv2D(512, kernel_size=(3, 3), activation="relu",  padding='same'),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dense(4096, activation="relu"),
        layers.Dropout(0.5),
        layers.Dense(4096, activation="relu"),
        layers.Dropout(0.5),
        layers.Dense(10, activation="softmax"),
    ]
)
model.summary()

batch_size = 256
epochs = 5


## data augmentation
x_train = tf.image.random_flip_up_down(x_train)
x_train = tf.image.random_flip_left_right(x_train)

model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(lr=0.00001), metrics=['accuracy'])
history = model.fit(x_train, y_train, batch_size=batch_size, epochs=5, verbose=1, validation_data=(x_test, y_test), steps_per_epoch=60000/batch_size)
